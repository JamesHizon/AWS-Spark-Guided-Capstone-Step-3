{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0d7f59",
   "metadata": {},
   "source": [
    "# Guided Capstone Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "814709f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a599c96f724811842e60cb90411854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sc.install_pypi_package(\"boto3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29042a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1593ffb8e07441bb80a190be209c4c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType\n",
    "import decimal\n",
    "import logging\n",
    "# import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4c15d4",
   "metadata": {},
   "source": [
    "# First, let us place all of the data into one folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bdf0e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89590967f798429ea39e6c17f74b6b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example S3 URI for later: \n",
    "# s3://guidedcapstonebucket2/e-KLOB2DYT8G9BGL5F8PJ9LXKJ/Ingested_CSV_Files/All_Ingested_CSV_Parquet/partition=T/part-00001-df714ede-7d68-427d-a5c2-cdbfd95cc993.c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a9fad81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c646402f5b487eb3e9ed356f29f6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the CSV Parser in Spark Transformation to Process the Source Data\n",
    "spark = SparkSession.builder.master('local').appName('app').getOrCreate()  # Main entry point to Spark cluster\n",
    "\n",
    "# # Create s3 resource object and use spark.conf.set() in order to work with S3 bucket.\n",
    "# s3_resource = boto3.resource('s3')\n",
    "\n",
    "spark.conf.set(\"fs.s3.endpoint\", \"s3-sa-east-1.amazonaws.com\")\n",
    "\n",
    "# Start Spark context manager --> used for creating RDD.\n",
    "sc = spark.sparkContext  # Entry point to Spark application w/ help of Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55a810aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f58e4dbca14bafa8d3809166799c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Populate trade dataset - first create output_path object\n",
    "output_path = \"s3://guidedcapstonebucket2/e-KLOB2DYT8G9BGL5F8PJ9LXKJ/Ingested_CSV_Files/All_Ingested_CSV_Parquet/\"\n",
    "\n",
    "# Create parquet file object for trade data\n",
    "trade_parquet_path = output_path + \"partition=T\"\n",
    "\n",
    "# Read Trade Partition Dataset from temp location\n",
    "trade_common = spark.read.parquet(trade_parquet_path)\n",
    "\n",
    "# Select Necessary Columns for Trade Records (Remove and filter out unnecessary columns)\n",
    "trade_df = trade_common.select(\"trade_dt\", \"symbol\", \"event_tm\", \"event_seq_nb\",\n",
    "                               \"trade_pr\",  \"arrival_tm\", \"trade_pr\")\n",
    "\n",
    "\n",
    "# Apply Data Correction\n",
    "def apply_latest_trade(df):\n",
    "    \"\"\"\n",
    "    Inside exchange dataset, you can uniquely identify a record by the combination of trade_dt,\n",
    "    symbol, exchange, event_tm, event_seq_nb.\n",
    "    However, the exchange may correct an error in any submitted record by sending a new record w/\n",
    "    the same unique ID.\n",
    "    --> Such records will come w/ later \"arrival_tm\". Thus, you must ensure you only accept the one w/\n",
    "    the most recent \"arrival_tm\".\n",
    "\n",
    "    LOGIC:\n",
    "     - If there are submitted records w/ same uniqueID, we will simply take the one w/ the\n",
    "     most recent arrival_tm.\n",
    "        ---> Operation involves aggregation to group dataset by unique ID s.t. a single function\n",
    "        can be applied to a certain group entirely.\n",
    "        ---> In Spark, we use groupBy operation to achieve this.\n",
    "\n",
    "    NOTE:\n",
    "    - I previously used \".show(1)\", but it can either be \".take(1)\" or what I currently have:\n",
    "    \".limit(1).show()\"\n",
    "    - Alternative way is to do a \".groupBy(<Specify columns>).agg(func(max(<\"arrival_tm\">))\".\n",
    "\n",
    "    :param df: Spark Dataframe\n",
    "    :return: After aggregating based on uniqueID, we will take the most recent \"arrival_tm\".\n",
    "    \"\"\"\n",
    "    most_recent_trade = df.groupBy(\"trade_dt\").agg(func.max(\"arrival_tm\").alias(\"max_arrival_dtm\"))\n",
    "    return most_recent_trade\n",
    "\n",
    "\n",
    "# Apply method\n",
    "trade_corrected = apply_latest_trade(trade_df)\n",
    "\n",
    "# Write Dataset Back To Parquet On AWS\n",
    "trade_date = \"2020-07-29\"\n",
    "trade_corrected.write.parquet(f\"{output_path}trade/trade_dt={trade_date}\")  # May have to manually create this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63a3f9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30af8eeb529412493d3018317cb7e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NEXT:\n",
    "# - Populate Quote dataset using same method.\n",
    "\n",
    "# Quote parquet path\n",
    "quote_parquet_path = output_path + \"partition=Q\"\n",
    "\n",
    "# Read Trade Partition Dataset from temp location\n",
    "quote_common = spark.read.parquet(quote_parquet_path)\n",
    "\n",
    "# Select Necessary Columns for Quote Records\n",
    "quote_df = quote_common.select(\"trade_dt\", \"symbol\", \"event_tm\", \"event_seq_nb\",\n",
    "                               \"trade_pr\",  \"arrival_tm\", \"trade_pr\")\n",
    "\n",
    "\n",
    "# Apply Data Correction\n",
    "def apply_latest_quote(df):\n",
    "    \"\"\"\n",
    "    Inside exchange dataset, you can uniquely identify a record by the combination of quote_dt,\n",
    "    symbol, exchange, event_tm, event_seq_nb.\n",
    "    However, the exchange may correct an error in any submitted record by sending a new record w/\n",
    "    the same unique ID.\n",
    "    --> Such records will come w/ later \"arrival_tm\". Thus, you must ensure you only accept the one w/\n",
    "    the most recent \"arrival_tm\".\n",
    "\n",
    "    LOGIC:\n",
    "     - If there are submitted records w/ same uniqueID, we will simply take the one w/ the\n",
    "     most recent arrival_tm.\n",
    "        ---> Operation involves aggregation to group dataset by unique ID s.t. a single function\n",
    "        can be applied to a certain group entirely.\n",
    "        ---> In Spark, we use groupBy operation to achieve this.\n",
    "\n",
    "    NOTE:\n",
    "    - I previously used \".show(1)\", but it can either be \".take(1)\" or what I currently have:\n",
    "    \".limit(1).show()\"\n",
    "\n",
    "    :param df: Spark Dataframe\n",
    "    :return: After aggregating based on uniqueID, we will take the most recent \"arrival_tm\".\n",
    "    \"\"\"\n",
    "    # Aggregate by unique id (use groupBy) --> Obtain most recent \"arrival_tm\"\n",
    "    most_recent_quote = df.groupBy(\"trade_dt\").agg(func.max(\"arrival_tm\").alias(\"max_arrival_dtm\"))\n",
    "    return most_recent_quote\n",
    "\n",
    "\n",
    "# Apply method\n",
    "quote_corrected = apply_latest_quote(quote_df)\n",
    "\n",
    "# Write Dataset Back To Parquet On AWS\n",
    "quote_date = \"2020-07-29\"\n",
    "quote_corrected.write.parquet(f\"{output_path}quote/quote_dt={quote_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85fb395",
   "metadata": {},
   "source": [
    "# Okay, we are done. Now, when we check S3 bucket, we should see are parquet files written as desired. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
